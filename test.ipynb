{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mloraw1/sfan/tunnel_llm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "%cd /mloraw1/sfan/tunnel_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTBase\n",
    "from data.utils import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Args ##\n",
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "class ModelConf:\n",
    "    def __init__(self, config):\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.dropout = config['dropout']\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_layer = config['n_layer']\n",
    "        self.n_embd = config['n_embd']\n",
    "        self.sequence_length = config['sequence_length']\n",
    "        self.bias = config['bias']\n",
    "\n",
    "def read_pkl(pkl_path:str):\n",
    "    with open(pkl_path, 'rb') as trg:\n",
    "        x = pickle.load(trg)\n",
    "    return x\n",
    "\n",
    "def load_config(summary_path):\n",
    "    print(\"\\nLoading config file\")\n",
    "    with open(summary_path) as fs:\n",
    "        config = json.load(fs)['args']\n",
    "    config = ModelConf(config)\n",
    "    print(f'{summary_path} loading complete!')\n",
    "    return config\n",
    "\n",
    "# Load the checkpoint\n",
    "def load_checkpoint(checkpoint_path, \n",
    "                    model_config:ModelConf, \n",
    "                    device='cpu',\n",
    "                    train=False,):\n",
    "    print(\"\\nLoading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    state_dict = checkpoint['model']\n",
    "    state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()} # distributed code appends '_orig_mod.' to param name\n",
    "    model = GPTBase(model_config)\n",
    "    model.load_state_dict(state_dict, strict=False) # olivia-add: strict=False, but do not understand why params fail to match\n",
    "    model.to(device)\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'vocab_size': 50304,\n",
    "    'dropout': 0.0,\n",
    "    'n_head': 12,\n",
    "    'n_embd': 768,\n",
    "    'sequence_length': 512,\n",
    "    'n_layer': 12,\n",
    "    'bias': 'false',\n",
    "    'dataset': 'redpajama-all',\n",
    "    'eval_all_domains': False,\n",
    "    'batch_size': 70,\n",
    "}\n",
    "# args = AttributeDict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset 'redpajama-all'\n",
      "Subset: arxiv || train: 30735125 || val: 1483751\n",
      "Subset: book || train: 25666343 || val: 1192070\n",
      "Subset: c4 || train: 165976168 || val: 8617730\n",
      "Subset: cc || train: 146088106 || val: 7781824\n",
      "Subset: github || train: 76541661 || val: 3878696\n",
      "Subset: stackexchange || train: 24287690 || val: 1271364\n",
      "Subset: wikipedia || train: 28937088 || val: 1579138\n",
      "Num training tokens: 498232181\n",
      "Num validation tokens: 25804573\n",
      "\n",
      "Loading checkpoint...\n",
      "number of parameters: 124.08M\n"
     ]
    }
   ],
   "source": [
    "args = AttributeDict(config_dict)\n",
    "model_config = ModelConf(config_dict)\n",
    "data = get_dataset(args)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "base_model = load_checkpoint(checkpoint_path='/mloraw1/sfan/curriculum-new/exps/slim_6b-all/base/124_baseline_base_lr0.0005_bs70x1_1nodes_seed=0/ckpt.pt', \n",
    "                        model_config=model_config, \n",
    "                        device='cuda', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPTBase.forward() missing 1 required positional argument: 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: GPTBase.forward() missing 1 required positional argument: 'idx'"
     ]
    }
   ],
   "source": [
    "base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "def sample_train_batch(data,\n",
    "                       seq_len=512,\n",
    "                       batch_size=4,\n",
    "                       device='cpu',):\n",
    "    span_ids = torch.arange(0, len(data) - seq_len, seq_len)\n",
    "    ix = torch.randint(len(span_ids), (batch_size,))\n",
    "    ix = span_ids[ix]\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+seq_len]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+seq_len]).astype(np.int64)) for i in ix])\n",
    "\n",
    "    if device != 'cpu':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x, y\n",
    "\n",
    "def sample_train_batch_from_dict(data,\n",
    "                                 seq_len=512,\n",
    "                                 batch_size=4,\n",
    "                                 device='cpu',\n",
    "                                 domain_weights=None,\n",
    "                                 idx2domain=None,\n",
    "                                 return_sample_dict=False,\n",
    "                                 return_domain_ids=False):\n",
    "    if domain_weights is None:\n",
    "        domain_weights = torch.ones(len(data), dtype=torch.float) / len(data)\n",
    "        idx2domain = {i:d for i,d in enumerate(list(data.keys()))}\n",
    "    assert len(domain_weights) == len(data)\n",
    "    assert len(idx2domain) == len(data)\n",
    "    \n",
    "    sampled_domain_ids = list(WeightedRandomSampler(weights=domain_weights, num_samples=batch_size, replacement=True))\n",
    "    idx2count = Counter(sampled_domain_ids)\n",
    "    print(idx2count)\n",
    "    \n",
    "    if return_sample_dict:\n",
    "        sample_dict = {}\n",
    "        for domain_id, domain_count in idx2count.items():\n",
    "            v = data[idx2domain[domain_id]]\n",
    "            span_ids = torch.arange(0, len(v) - seq_len, seq_len)\n",
    "            ix = torch.randint(len(span_ids), (domain_count,))\n",
    "            ix = span_ids[ix]\n",
    "            domain_x = torch.stack([torch.from_numpy((v[i:i+seq_len]).astype(np.int64)) for i in ix])\n",
    "            domain_y = torch.stack([torch.from_numpy((v[i+1:i+1+seq_len]).astype(np.int64)) for i in ix])\n",
    "            if device != 'cpu':\n",
    "                # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "                domain_x, domain_y = domain_x.pin_memory().to(device, non_blocking=True), domain_y.pin_memory().to(device, non_blocking=True)\n",
    "            sample_dict[domain_id] = (domain_x, domain_y)\n",
    "        return sample_dict, idx2domain\n",
    "    \n",
    "    id_list = []\n",
    "    for domain_id, domain_count in idx2count.items():\n",
    "        v = data[idx2domain[domain_id]]\n",
    "        span_ids = torch.arange(0, len(v) - seq_len, seq_len)\n",
    "        ix = torch.randint(len(span_ids), (domain_count,))\n",
    "        ix = span_ids[ix]\n",
    "        id_list.extend([(domain_id, i) for i in ix])\n",
    "    \n",
    "    x = torch.stack([torch.from_numpy((data[idx2domain[domain_id]][i:i+seq_len]).astype(np.int64)) for (domain_id, i) in id_list])\n",
    "    y = torch.stack([torch.from_numpy((data[idx2domain[domain_id]][i+1:i+1+seq_len]).astype(np.int64)) for (domain_id, i) in id_list])\n",
    "    \n",
    "    if device != 'cpu':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    if return_domain_ids:\n",
    "        sampled_domain_ids = torch.LongTensor([id for (id, _) in id_list])\n",
    "        token_domain_ids = torch.zeros(len(sampled_domain_ids)*seq_len, dtype=torch.long)\n",
    "        for id in sampled_domain_ids:\n",
    "            token_domain_ids[id*seq_len:(id+1)*seq_len] = domain_id\n",
    "        return x, y, sampled_domain_ids, token_domain_ids\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 7, 5: 6, 3: 5, 2: 5, 1: 3, 0: 3, 7: 2, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "x, y = sample_train_batch_from_dict(data['train'],\n",
    "                                        seq_len=128,\n",
    "                                        batch_size=32,\n",
    "                                        device='cuda',\n",
    "                                        domain_weights=None,\n",
    "                                        idx2domain=None,\n",
    "                                        return_sample_dict=False,\n",
    "                                        return_domain_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = base_model(x, y, return_layer_rep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_reps = output['layer_reps']\n",
    "# len(layer_reps) # 6\n",
    "# layer_reps[0].shape # [64, 128, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_reps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 7, 7: 6, 6: 6, 5: 4, 3: 3, 0: 2, 2: 2, 4: 2})\n",
      "Counter({5: 8, 7: 5, 3: 4, 0: 4, 2: 3, 4: 3, 6: 3, 1: 2})\n",
      "Counter({5: 6, 0: 6, 4: 5, 1: 5, 2: 4, 7: 3, 6: 2, 3: 1})\n",
      "Counter({7: 9, 3: 5, 5: 4, 0: 4, 1: 4, 2: 3, 4: 2, 6: 1})\n",
      "Counter({5: 9, 0: 6, 3: 6, 1: 4, 2: 3, 7: 2, 6: 1, 4: 1})\n",
      "Counter({6: 6, 1: 6, 7: 4, 0: 4, 5: 4, 3: 3, 4: 3, 2: 2})\n",
      "Counter({4: 7, 2: 5, 7: 5, 5: 5, 1: 3, 3: 3, 0: 3, 6: 1})\n",
      "Counter({2: 6, 6: 6, 5: 6, 3: 4, 0: 4, 4: 4, 1: 2})\n",
      "Counter({7: 7, 5: 7, 2: 5, 4: 5, 3: 2, 1: 2, 0: 2, 6: 2})\n",
      "Counter({5: 6, 0: 6, 1: 5, 4: 4, 7: 4, 6: 3, 2: 2, 3: 2})\n"
     ]
    }
   ],
   "source": [
    "from torch.linalg import matrix_rank\n",
    "\n",
    "rank_dict = {idx:[] for idx in range(args.n_layer)}\n",
    "\n",
    "for t in range(10):\n",
    "    x, y = sample_train_batch_from_dict(data['train'],\n",
    "                                        seq_len=128,\n",
    "                                        batch_size=32,\n",
    "                                        device='cuda',\n",
    "                                        domain_weights=None,\n",
    "                                        idx2domain=None,\n",
    "                                        return_sample_dict=False,\n",
    "                                        return_domain_ids=False)\n",
    "    output = base_model(x, y, return_layer_rep=True)\n",
    "    layer_reps = output['layer_reps']\n",
    "    cur_rank_dict = {layer_idx: matrix_rank(torch.cov(rep.flatten(start_dim=0, end_dim=1).T)).item()\n",
    "                    for layer_idx, rep in enumerate(layer_reps)}\n",
    "    for k in rank_dict.keys():\n",
    "        rank_dict[k].append(cur_rank_dict[k])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [630, 597, 606, 602, 592, 621, 588, 616, 617, 616],\n",
       " 1: [545, 523, 530, 524, 518, 544, 518, 532, 538, 532],\n",
       " 2: [657, 635, 643, 638, 632, 660, 629, 642, 648, 643],\n",
       " 3: [760, 748, 752, 749, 745, 761, 744, 753, 752, 753],\n",
       " 4: [767, 767, 767, 767, 767, 767, 767, 767, 767, 767],\n",
       " 5: [624, 604, 599, 596, 595, 615, 598, 612, 606, 610],\n",
       " 6: [687, 668, 664, 657, 661, 676, 663, 677, 669, 673],\n",
       " 7: [761, 753, 750, 746, 748, 759, 750, 760, 756, 757],\n",
       " 8: [767, 767, 767, 767, 767, 767, 767, 767, 767, 767],\n",
       " 9: [768, 768, 768, 768, 768, 768, 768, 768, 768, 768],\n",
       " 10: [768, 768, 768, 767, 768, 768, 768, 768, 768, 768],\n",
       " 11: [767, 767, 767, 767, 767, 767, 767, 767, 767, 767]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_size: int, \n",
    "                 output_size: int,\n",
    "                 iterations: int=1000,\n",
    "                 lr: float=5e-4,\n",
    "                 opt = None):\n",
    "        super().__init__()\n",
    "        self.linear_probe = nn.Linear(feature_size, output_size)\n",
    "        if opt is None:\n",
    "            self.optimizer = torch.optim.Adam(self.linear_probe.parameters(), lr=lr)\n",
    "        else:\n",
    "            self.optimizer = opt\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.total_iterations = iterations\n",
    "        self.best_val_acc = 0.0\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            self.linear_probe = self.linear_probe.cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_probe(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
